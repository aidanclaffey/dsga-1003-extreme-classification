{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAFTML\n",
    "\n",
    "The CRAFTML Rust implementation can be found here: https://github.com/tomtung/craftml-rs\n",
    "\n",
    "And the paper can be found here: http://proceedings.mlr.press/v80/siblini18a/siblini18a.pdf\n",
    "\n",
    "To run the code:\n",
    "\n",
    "1. Install Rust from the website here: https://www.rust-lang.org/\n",
    "2. Then, clone the github into a local directory adjacent to this one (so that they both have the same parent folder).\n",
    "3. Follow the instruction on the github to build the rust program\n",
    "4. Download the Eurlex 4k dataset from the Extreme Multilabel repository (or use the Eurlex data folder in our project github)\n",
    " 1.  If you downloaded it from the extreme multilabel repository, make sure to run the cross validation code (the second code cell) to build the cv data sets. If not, then skip this code cell.\n",
    "5. Then, you should be able to run the rest of the code just fine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "import pickle\n",
    "from itertools import cycle, product\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "import time\n",
    "\n",
    "num_features = 5000\n",
    "num_labels = 3993\n",
    "num_samples_val = 3809\n",
    "num_samples_train = 15539\n",
    "\n",
    "cv_idx = np.random.randint(5, size = num_samples_train)\n",
    "cv_counts = np.unique(cv_idx, return_counts = True)[1]\n",
    "results = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code block sets up the cross validation folds in the format that the craftml implementation needs.\n",
    "Each input file should be of the form:\n",
    "```\n",
    "num_examples num_total_features num_total_labels\n",
    "label1,label2,... feature1:score1 feature2:score2 ...\n",
    "```\n",
    "\n",
    "Which is how the txt file looks like if you download it from the Extreme Classification Repository http://manikvarma.org/downloads/XC/XMLRepository.html\n",
    "\n",
    "All of the cross validation data sets are saved in the Eurlex folder (along with the train and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write first line to each cv val and train file\n",
    "# # first line is 'num_samples num_features num_labels'\n",
    "# for i in range(5):\n",
    "#     file_name = 'Eurlex/cv_' + str(i) + '_val'\n",
    "#     with open(file_name, 'w') as file:\n",
    "#         file.write(str(cv_counts[i]) + ' ' + str(num_features) + ' ' + str(num_labels) + '\\n')\n",
    "        \n",
    "#     file_name = 'Eurlex/cv_' + str(i) + '_train'\n",
    "#     with open(file_name, 'w') as file:\n",
    "#         file.write(str(num_samples_train - cv_counts[i]) + ' ' + str(num_features) +\n",
    "#                    ' ' + str(num_labels) + '\\n')\n",
    "\n",
    "# with open('Eurlex/eurlex_train.txt', 'r') as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         if(i == 0):\n",
    "#             continue\n",
    "#         for j in range(5):\n",
    "#             if j == cv_idx[i-1]:\n",
    "#                 file_name = 'Eurlex/cv_' + str(j) + '_val'\n",
    "#             else:\n",
    "#                 file_name = 'Eurlex/cv_' + str(j) + '_train'\n",
    "#             with open(file_name, 'a') as file:\n",
    "#                 file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using 5 fold cross validation and grid-search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 100 0 0.5606432932442618\n",
      "10 100 1 0.5668999217640313\n",
      "10 100 2 0.5665334665663533\n",
      "10 100 3 0.5735650490041243\n",
      "10 100 4 0.5673084160012928\n",
      "time: 405.85119247436523\n"
     ]
    }
   ],
   "source": [
    "train_cmd_call = '../craftml-rs/target/release/craftml train --model_path model.m'\n",
    "val_cmd_call = '../craftml-rs/target/release/craftml test --out_path predictions.txt --k_top ' + str(num_labels) + ' model.m '\n",
    "k_clusters_list = [10]\n",
    "n_trees_list = [100]\n",
    "cluster_sample_list = [1000]\n",
    "centroid_min_n_preserve_list = [500]\n",
    "hyper_parameter_grid = product(k_clusters_list, n_trees_list, cluster_sample_list, centroid_min_n_preserve_list)\n",
    "\n",
    "start = time.time()\n",
    "for k_clusters, n_trees, cluster_size, centroid_min in hyper_parameter_grid:\n",
    "    \n",
    "    # build the command line call to train the model\n",
    "    cv_cmd_train = train_cmd_call + ' --k_clusters ' + str(k_clusters) + ' --n_trees ' + str(n_trees) + \\\n",
    "                    ' --cluster_sample_size ' + str(cluster_size) +' --centroid_min_n_preserve ' + \\\n",
    "                    str(centroid_min) + ' --n_feature_buckets 1000 --n_label_buckets 1000'\n",
    "    # results for each cv-fold\n",
    "    param_results = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        train_file_name = 'Eurlex/cv_' + str(i) + '_train'\n",
    "        val_file_name = 'Eurlex/cv_' + str(i) + '_val'\n",
    "        # add the correct cv file to the end of the cmd line call for train\n",
    "        cv_ith_cmd_train = cv_cmd_train + ' ' + train_file_name\n",
    "        # call the command\n",
    "        os.system(cv_ith_cmd_train)\n",
    "        # add correct cv file to the end of the cmd line for test\n",
    "        cv_ith_cmd_val = val_cmd_call + val_file_name\n",
    "        # call the command\n",
    "        os.system(cv_ith_cmd_val)\n",
    "        # create the predictions matrix \n",
    "        predictions = np.zeros((cv_counts[i], num_labels))\n",
    "        with open('predictions.txt') as f:\n",
    "            for j, line in enumerate(f):\n",
    "                for val in line.split(sep = '\\t'):\n",
    "                    label, prob = val.split(sep = ' ')\n",
    "                    label = int(label)\n",
    "                    prob = float(prob)\n",
    "                    predictions[j, label] = prob\n",
    "        \n",
    "        # create the true labels matrix\n",
    "        true_labels = np.zeros((cv_counts[i], num_labels))\n",
    "        with open(val_file_name, 'r') as f:\n",
    "            for j, line in enumerate(f):\n",
    "                if(j == 0):\n",
    "                    continue\n",
    "                label_string = line.split(sep = ' ')[0]\n",
    "                if(label_string == ''):\n",
    "                    continue\n",
    "                labels = label_string.split(sep = ',')\n",
    "                for label in labels:\n",
    "                    true_labels[j-1,int(label)] = 1\n",
    "        # take the LRAP score\n",
    "        lrap_cv = label_ranking_average_precision_score(true_labels, predictions)\n",
    "        print(k_clusters, n_trees, i, lrap_cv)\n",
    "        param_results[i] = lrap_cv\n",
    "    \n",
    "    dict_key = str(k_clusters) + ',' + str(n_trees) + ',' + str(cluster_size) + ',' + str(centroid_min) + ' 2'\n",
    "    results[dict_key] = param_results.mean()\n",
    "print('time:', time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10,100,1000,500 2': 0.5669900293160126}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing dev dataset on the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5895317770806462\n",
      "time: 183.3079273700714\n"
     ]
    }
   ],
   "source": [
    "# best parameters: 200 trees, max_leaf 10, centroid preserve 500, cluster sample size 1000\n",
    "# n_feature buckets 1000, n_label_buckets 1000 (these last two are cited in the paper to be the best for the\n",
    "# Eurlex 4K dataset)\n",
    "train_cmd_call = '../craftml-rs/target/release/craftml train --model_path model.m --cluster_sample_size 1000'+\\\n",
    "                 ' --centroid_min_n_preserve 500 --leaf_max_size 10 --n_trees 200' +\\\n",
    "                 ' --n_feature_buckets 1000 --n_label_buckets 1000 Eurlex/eurlex_train.txt'\n",
    "val_cmd_call = '../craftml-rs/target/release/craftml test --out_path predictions.txt --k_top ' +\\\n",
    "                str(num_labels) + ' model.m Eurlex/dev.txt'\n",
    "start = time.time()\n",
    "os.system(train_cmd_call)\n",
    "os.system(val_cmd_call)\n",
    "predictions = np.zeros((1316, num_labels))\n",
    "with open('predictions.txt') as f:\n",
    "    for j, line in enumerate(f):\n",
    "        for val in line.split(sep = '\\t'):\n",
    "            label, prob = val.split(sep = ' ')\n",
    "            label = int(label)\n",
    "            prob = float(prob)\n",
    "            predictions[j, label] = prob\n",
    "\n",
    "true_labels = np.zeros((1316, num_labels))\n",
    "with open('Eurlex/dev.txt', 'r') as f:\n",
    "    for j, line in enumerate(f):\n",
    "        if(j == 0):\n",
    "            continue\n",
    "        label_string = line.split(sep = ' ')[0]\n",
    "        if(label_string == ''):\n",
    "            continue\n",
    "        labels = label_string.split(sep = ',')\n",
    "        for label in labels:\n",
    "            true_labels[j-1,int(label)] = 1\n",
    "lrap= label_ranking_average_precision_score(true_labels, predictions)\n",
    "print(lrap)\n",
    "print('time:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
